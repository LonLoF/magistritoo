# -*- coding: utf-8 -*-
"""Magistritöö

Automatically generated by Colab.
"""

# @title Üldised teegid

import pandas as pd
import numpy as np

"""## Eeltöötlus"""

# @title Andmete sisse lugemine
bacters = pd.read_csv('01122023_bakterid.csv')
surfaces = pd.read_csv('pinnad 12.01.2024.csv', dtype={'Comment': str})

raw_data = pd.concat([bacters, surfaces], ignore_index=True)
print(f'Raw data size: {len(raw_data)}')

# @title Andmete sorteerimine ja statistika
# @markdown Eraldame bakterite, pindade, phuvri ja sööda ning tundmatud
statistika = True # @param {type:"boolean"}
letters_to_numbers = {
    'a': 3,
    'b': 4,
    'c': 13,
    'd': 5,
    'e': 6,
    'f': 8,
    'g': 7,
    'h': 2,
    'i': 1
}

def statistics(df):
    print('-' * 60)
    print(f"Surfaces: {df['Surface'].unique()}")
    print(f"Bacteria: {df['Bacteria'].unique()}")

    total = len(df)
    print(f'\nTotal count: {total}')
    print(df.pivot_table(index='Surface', columns='Bacteria', aggfunc='size', fill_value=0))

    count = len(df[df['Concentration'] == 1])
    if total != count:
        print(f'\nConcentration 1 count: {count}')
        print(df[df['Concentration'] == 1].pivot_table(index='Surface', columns='Bacteria', aggfunc='size', fill_value=0))

        count2 = len(df[df['Concentration'] == 0.1])
        print(f'\nConcentration 0.1 count: {count2}')
        print(df[df['Concentration'] == 0.1].pivot_table(index='Surface', columns='Bacteria', aggfunc='size', fill_value=0))
    #print('-' * 60)

# Separate surface data
def can_convert_to_int(s):
    try:
        int(s)
        return True
    except ValueError:
        return False

def pre_process(df, stats=statistika):
    # Forward fill the NaN values in the 'Comment' column
    df['Comment'] = df['Comment'].ffill().str.lower()
    print(f'Raw data size: {len(df)}')

    # Use the boolean mask to filter the DataFrame and keep rows without 0 values
    filtered_df = df[~(df == 0).any(axis=1)].copy()
    print(f'Raw data size after removing 0 values: {len(df)}')

    # Separate data
    def separate(filter, name=None, concentration=1):
        nonlocal filtered_df
        separated = filtered_df.loc[filter].copy()
        separated['Concentration'] = concentration
        separated['Surface'] = separated['Comment'].str.replace('p', '').str.replace('s', '')
        separated = separated.drop(columns=['Name', 'Comment'])
        if name is not None:
            separated['Bacteria'] = name
        filtered_df = filtered_df.drop(separated.index)
        return separated

    # Separate validation data
    unkonown_samples = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']
    unkonown_set = separate(filtered_df['Comment'].isin(unkonown_samples))
    unkonown_set['Bacteria'] = unkonown_set['Surface'].map(letters_to_numbers) #unkonown_set['Surface']
    unkonown_set['Surface'] = 2

    surface_set = separate(filtered_df['Comment'].apply(can_convert_to_int), 'Surface')
    surface_set['Surface'] = surface_set['Surface'].astype(int)

    # Separate feed and buffer solution
    feed_set = separate(filtered_df['Comment'].str.match(r'^\d+s'), 'Feed')
    feed_set['Surface'] = feed_set['Surface'].astype(int)

    buffer_set = separate(filtered_df['Comment'].str.match(r'^\d+p'), 'Buffer')
    buffer_set['Surface'] = buffer_set['Surface'].astype(int)

    # Create 'Concentration' column based on the presence of 'l' in 'Comment'
    filtered_df['Concentration'] = filtered_df['Comment'].apply(lambda x: 0.1 if 'l' in x else 1)
    filtered_df = filtered_df[(filtered_df['Concentration'] == 1)]

    # Replace 'p' with an empty string and 'b' with '-' to get the same string structure
    filtered_df['Comment'] = filtered_df['Comment'].str.replace('p', '').str.replace('b', '-').str.replace('l', '')

    # Split the 'Comment' column and create new columns 'Surface' and 'Bacteria'
    filtered_df[['Surface', 'Bacteria']] = filtered_df['Comment'].str.split('-', n=1, expand=True)

    filtered_df['Surface'] = filtered_df['Surface'].astype(int)
    filtered_df['Bacteria'] = filtered_df['Bacteria'].astype(int)

    # Drop columns 'Name' and 'Comment'
    filtered_df = filtered_df.drop(columns=['Name', 'Comment'])

    filtered_df = pd.concat([filtered_df, unkonown_set])

    if stats:
        statistics(filtered_df)
        #statistics(unkonown_set)
        statistics(surface_set)
        statistics(feed_set)
        statistics(buffer_set)

    return filtered_df, surface_set, feed_set, buffer_set

bacteria_df, surface_df, feed_df, buffer_df = pre_process(raw_data)

"""### Statistiliste kõrvalkallete eemaldamine"""

# Function to identify outliers using IQR
columns_to_check = ["340-405", "340-460", "310-340", "310-405", "310-460", "280-340", "280-405", "280-460"]
def iqr_outliers(group):
    Q1 = group[columns_to_check].quantile(0.25)
    Q3 = group[columns_to_check].quantile(0.75)
    IQR = Q3 - Q1
    iqr_threshold = 1.3
    outliers = ((group[columns_to_check] < (Q1 - iqr_threshold * IQR)) | (group[columns_to_check] > (Q3 + iqr_threshold * IQR)))
    return outliers.any(axis=1)

for data in [bacteria_df, surface_df, feed_df, buffer_df]:
    # Group the DataFrame by "Surface" and "Bacteria" columns
    grouped_df = data.groupby(["Surface", "Bacteria"], group_keys=True)
    size = grouped_df.size().rename("Beginning size")

    # Identify outliers and get indexes
    outlier_mask = grouped_df.apply(iqr_outliers)
    outlier_indexes = [index[2] for index in outlier_mask[outlier_mask > 0].index]

    # Mark outliers in the 'Surface' column
    data.loc[outlier_indexes, 'Surface'] = 'Outlier'
    pd.set_option('display.max_rows', 100)
    print(pd.concat([size, data.groupby(["Surface", "Bacteria"], group_keys=True).size().rename("Ending size")[:len(size)]], axis=1))

"""## Tunnuste lisamine"""

# @title Loome pinna tunnused ja rikastame nendega bakteri mõõtmisi

surface_mean_df = surface_df.drop(columns=['Bacteria'])
surface_mean_df = surface_mean_df.groupby('Surface').mean()
surface_mean_df = surface_mean_df.drop(columns='Concentration')
surface_mean_df.columns = [f'{col}_surface' if col != 'Surface' else col for col in surface_mean_df.columns]

enriched_df = pd.merge(bacteria_df[(bacteria_df['Surface'] != 'Outlier') & (bacteria_df['Concentration'] == 1)], surface_mean_df, on='Surface', how='left')

# @title Ühekordne kodeerimine pinna tunnusele

dummy_columns = pd.get_dummies(enriched_df['Surface'], prefix='Surface', drop_first=False)
enriched_df = pd.concat([enriched_df, dummy_columns], axis=1)

# @title Lahutame baterite spektrist pinna spektri

channels = ['340-405', '340-460', '310-340', '310-405', '310-460', '280-340', '280-405', '280-460']

for channel in channels:
    surface_col_name = channel + '_surface'
    result_col_name = channel + '_B-S'
    enriched_df[result_col_name] = enriched_df[channel] - enriched_df[surface_col_name]

"""## Treenimine"""

# @title Teegid
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from collections import Counter, defaultdict

from datetime import datetime

# @title Jagame andmed tunnusteks, ennustatavaks ja määrame jagamise kombinatsioonid, et igat bakteri ja pinna kombinatsiooni saaks igasse foldi
enriched_df['Combination'] = enriched_df['Surface'].astype(str) + '_' + enriched_df['Bacteria'].astype(str)

final_df = enriched_df[enriched_df['Bacteria'] != 12]
final_df = final_df.reset_index(drop=True)

X = final_df.drop(columns=['Surface', 'Concentration', 'Bacteria', 'Combination'])
y = final_df['Bacteria']
combination_labels =  final_df['Combination']

from sklearn.ensemble import VotingClassifier

kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)


def train_model(X_train_scaled, y_train, estimator, param_grid):
    # Train a model using GridSearchCV
    grid_search = GridSearchCV(
        estimator=estimator,
        param_grid=param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        verbose=2
    )
    grid_search.fit(X_train_scaled, y_train)
    print(grid_search.best_params_)

    return grid_search.best_estimator_


def store_metrics(model_metrics, name, model, X_test_scaled, y_test):
    #Evaluate
    y_pred = model.predict(X_test_scaled)

    score = accuracy_score(y_test, y_pred)

    report = classification_report(y_test, y_pred, output_dict=True)

    cm = confusion_matrix(y_test, y_pred)
    accuracies = np.zeros(len(cm))

    # Calculate class-specific accuracies
    for i in range(len(cm)):
        TP = cm[i, i]
        FP = cm[:, i].sum() - TP
        FN = cm[i, :].sum() - TP
        TN = cm.sum() - (TP + FP + FN)
        accuracies[i] = (TP + TN) / cm.sum()

    # Store metrics
    model_metrics[name]['Over all']['scores'].append(score)
    for class_name, idx in zip(np.unique(y_test), range(len(accuracies))):
        class_name = str(class_name)
        model_metrics[name][class_name]['precision'].append(report[class_name]['precision'])
        model_metrics[name][class_name]['recall'].append(report[class_name]['recall'])
        model_metrics[name][class_name]['f1-score'].append(report[class_name]['f1-score'])
        model_metrics[name][class_name]['accuracy'].append(accuracies[idx])

    return model_metrics, score, cm


def trainer(models_with_params):
    start_time = datetime.now()

    # Dictionary to store metrics for each model, each class
    model_metrics = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
    all_confusion_matrices = defaultdict(list)

    # Main loop over the folds of the outer k-fold cross-validation
    for fold, (train_index, test_index) in enumerate(kf.split(X, combination_labels)):
        print('Warning info: The issue concerns the "combination_labels", where some combinations of surface and bacteria do not have sufficient data, though each class individually does.\n')
        print(f"\nFold {fold+1}")

        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Train each model and store the best estimators
        best_estimators = []
        accuracies = []
        fold_confusion_matrices = {}
        for name, (estimator, param_grid) in models_with_params.items():
            model_start_time = datetime.now()
            best_model = train_model(X_train_scaled, y_train, estimator, param_grid)
            best_estimators.append((name, best_model))
            
            model_metrics, score, cm = store_metrics(model_metrics, name, best_model, X_test_scaled, y_test)
            accuracies.append(score)
            fold_confusion_matrices[name] = cm
            print(f'Model {name} training time: {datetime.now() - model_start_time}\n')
        
        # Weights for each model
        total_accuracy = sum(accuracies)
        weights = [accuracy / total_accuracy for accuracy in accuracies]
        
        # Create and evaluate the voting classifier
        model_start_time = datetime.now()
        voting_clf = VotingClassifier(estimators=best_estimators, voting='soft', weights=weights)
        voting_clf.fit(X_train_scaled, y_train)

        model_metrics, _, cm = store_metrics(model_metrics, 'Ansable Voting', voting_clf, X_test_scaled, y_test)
        fold_confusion_matrices['Ansable Voting'] = cm
        all_confusion_matrices[fold] = fold_confusion_matrices
        print(f'Model Ansable Voting training time: {datetime.now() - model_start_time}')

    print(f'\nTotal time: {datetime.now() - start_time}')
    # Output averaged metrics for each model, each class
    print('\nFinal Average Metrics Per Model and Class:')
    for model, classes in model_metrics.items():
        print(f"\n{model}:")
        for class_name, metrics in classes.items():
            if class_name != 'Over all':
                avg_precision = np.mean(metrics['precision'])
                avg_recall = np.mean(metrics['recall'])
                avg_f1 = np.mean(metrics['f1-score'])
                avg_accuracy = np.mean(metrics['accuracy'])
                print(f'{class_name} - Precision: {avg_precision:.3f}, Recall: {avg_recall:.3f}, F1-Score: {avg_f1:.3f}, Accuracy: {avg_accuracy:.3f}')
            else:
                mean_score = np.mean(metrics['scores'])
                std_score = np.std(metrics['scores'])
                print(f'Average accuracy over folds: {mean_score:.3f} ± {std_score:.3f}')
    
    total_confusion_matrices = {}
    for fold, fold_matrices in all_confusion_matrices.items():
        for model_name, cm in fold_matrices.items():
            if model_name not in total_confusion_matrices:
                total_confusion_matrices[model_name] = cm
            else:
                total_confusion_matrices[model_name] += cm

    print('\nTotal Confusion Matrices Per Model:')
    for model_name, total_cm in total_confusion_matrices.items():
        print(f'\nModel {model_name}:\n{total_cm}')

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)
knn = KNeighborsClassifier()
svc = SVC(random_state=42, probability=True)

# The grid of hyperparameters to search
dt_param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'max_features': [None, 'sqrt', 'log2']
}

rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [None, 'sqrt'],
}

knn_param_grid = {
    'n_neighbors': [3, 5, 7, 10],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']
}

svc_param_grid = {
    'C': [5, 10, 15],
    'gamma': ['scale', 'auto'],
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'degree': [5, 6, 7, 8],
    'coef0': [0.5, 1.0, 1.5]
}


models_with_params = {
    'DT': (dt, dt_param_grid),
    'RF': (rf, rf_param_grid),
    'KNN': (knn, knn_param_grid),
    'SVC': (svc, svc_param_grid),
}


trainer(models_with_params)
